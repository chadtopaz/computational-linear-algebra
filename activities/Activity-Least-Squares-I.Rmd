---
title: "Activity - Least Squaresa I"
output:
  rmarkdown::md_document:
  header-includes:
   - \usepackage{amsmath}
---

\renewcommand{\vec}[1]{\mathbf{#1}}
\renewcommand{\mat}[1]{\mathbf{#1}}

Remember to show your work / give your reasoning, as applicable.

```{r message=FALSE}
library(pracma)
```

### Problem 1

Let

$$
\vec{a}_1 = \begin{pmatrix} 0 \\ 3 \\ 4 \end{pmatrix}, \quad \vec{a}_2 = \begin{pmatrix} 2 \\ 2 \\ 1 \end{pmatrix}
$$

be the columns of the 3x2 matrix $\mat{A}$. Let ${\cal W}=\hbox{col}(\mat{A})=\hbox{span}\{\vec{a}_1,\vec{a}_2\}$, and let

$$
\vec{b} = \begin{pmatrix} 2 \\ -1 \\ 1 \end{pmatrix}.
$$

In this problem, any time you need a norm, use the Euclidean norm.

a\. Find the projection of $\vec{b}$ onto $\hbox{span}\{\vec{a}_1\}$. Call that $\widehat{\vec{b}}_1$.

b\. Find the projection of $\vec{b}$ onto $\hbox{span}\{\vec{a}_2\}$. Call that $\widehat{\vec{b}}_2$.

c\. Compute the pseudoinverse of $\mat{A}$.

d\. Compute the projection operator $\mat{P}_{{\cal W}}$ such that the closest point to a given vector $\vec{v} \in \mathbb{R}^3$ in the space ${\cal W}$ is $\vec{P}_{{\cal W}}\vec{v}$.

e\. Find the vector $\widehat{\vec{b}} \in {\cal W}$ that is closest to $b$.

f\. TRUE or FALSE: $\widehat{\vec{b}}=\widehat{\vec{b}}_1 + \widehat{\vec{b}}_2$. When is this true in general? When is it false?

g\. Let ${\cal W}^{\perp}$ be the orthogonal complement of $W$; i.e., every vector in ${\cal W}^{\perp}$ is orthogonal to every vector in $W$. What is the dimension of ${\cal W}^{\perp}$ for this problem?

h\. Draw an abstract picture (don't worry about getting the placement of the vectors correct) and label $\vec{a}_1$, $\vec{a}_2$, ${\cal W}$, ${\cal W}^{\perp}$, $\vec{b}$, $\widehat{\vec{b}}_1$, $\widehat{\vec{b}}_2$, $\widehat{\vec{b}}$, and $\vec{r}=\vec{b}-\widehat{\vec{b}}$.

i\. Find vectors $\vec{v}_1$ and $\vec{v}_2$ in $\mathbb{R}^3$ such that these three conditions are all met:

  -   $\vec{b}=\vec{v}_1+\vec{v}_2$
  -   $\vec{v}_1 \in {\cal W}$
  -   $\vec{v}_2 \in {\cal W}^{\perp}$

j\. What is the distance from $\vec{b}$ to ${\cal W}$?

### Problem 1 Solution

a\. Your solution goes here.

b\. Your solution goes here.

c\. Your solution goes here.

d\. Your solution goes here.

e\. Your solution goes here.

f\. Your solution goes here.

g\. Your solution goes here.

h\. Your solution goes here.

i\. Your solution goes here.

j\. Your solution goes here.


### Problem 2

Consider the points:

```{r cache = TRUE}
t <- 1:8
y <- c(1,3,2,5,5,7,7,7)
plot(t, y, col = "red", pch = 20, cex = 2, xlim = c(1,8), ylim = c(0,8))
grid()
```

a\. Setup the normal equations and solve them with R's `solve` function in order to find the least squares linear fit for this data. Plot your regression line on the same plot with the points.

b\. Setup and solve the normal equations in order to find the least squares parabola fit (i.e., polynomial of degree 2) for this data. Plot your best fit curve on the same plot with the points.

c\. Do you expect $||\mat{A}\,\vec{x}-\vec{b}||_2^2$ (which we call "the sum of squared residuals") to be lower for part (a) or part (b)? Why? Explain in plain English and using linear algebra vocabulary. Does your answer depend on the set of data given to you? Compute the sum of squared residuals for part (a) and part (b) to see if it matches your intuition.

### Problem 2 Solution

a\. Your solution goes here.

b\.  Your solution goes here.

c\.  Your solution goes here.

### Problem 3

In this exercise, we go through the details of using optimization to show that a vector $\vec{x}$ that satisfies the normal equations represents the coefficients of the least squares solution. 

a\. The least squares error term

$$
\frac{1}{2} ||\mat{A}\vec{x}-\vec{b}||_2^2
$$

can be written as a quadratic function 

$$
f(\vec{x})=\frac{1}{2}\vec{x}^{\top}\mat{P}\vec{x} + \vec{q}^{\top}\vec{x}+\vec{c}.
$$

Put the error term in this form, that is, find the matrix $\mat{P}$, vector $\vec{q}$ and constant vector $\vec{c}$ in terms of $\mat{A}$ and $\vec{b}$. Hint: remember that for any vector $\vec{v}$, $||\vec{v}||_2^2=\vec{v}^{\top}\vec{v}$.

b\. Show that the $\mat{P}$ matrix from part (a) is symmetric and positive semidefinite. Feel free to look up what positive semidefinite means!

c\. Here's a fact: if the $\mat{P}$ matrix in the quadratic function form above is symmetric positive semidefinite, then $\nabla f(\vec{x})=\mat{P}\,\vec{x}+\vec{q}$, and the critical point $\vec{x}$ satisfying $\nabla f (\vec{x})=\vec{0}$ is a global minimum. Conclude that the value of $\vec{x}$ that satisfies the normal equations gives the optimal least squares coefficients.

### Problem 3 Solution

a\. Your solution goes here.

b\. Your solution goes here.

c\. Your solution goes here.