---
title: "Activity - Eigenvalues"
author: "Insert group members' full names here"
output: rmarkdown::github_document
---

\renewcommand{\vec}[1]{\mathbf{#1}}
\renewcommand{\mat}[1]{\mathbf{#1}}

```{r message = FALSE, echo = FALSE}
library(pracma)
library(expm)
```

### Procedure

* Please collaborate on this activity with one person as scribe (typist). There should only be one laptop open. We're aiming for actual collaboration here, not two or three people working in parallel.

* It doesn't matter if you finish the whole activity during class.

* It **does** mater if you focus on helping each other learn. Helping each other learn is the point of this activity. Completing the problems is merely a mechanism to guide you towards learning.

* It **does** mater if everyone participates. The expectation is that every group member participates, where the definition of "participation" can include asking questions, answering questions, brainstorming, or any other form of meaningful engagement.

* In the last three minutes of class, before you leave, please knit your .Rmd file to html, print the html to .pdf, and have the scribe submit it as a group assignment through GLOW.

* Don't forget that the solutions are available on GLOW.

### Problem 0

Recalling that academic integrity policies apply even in an ungraded course, please disclose any irregular circumstances related to participation in your group (for instance, a group member was missing and did not contribute to this document).

### Problem 0 Solution

Your solution goes here.

### Problem 1

Let 
$$
\mat{A}=\frac{1}{9}\begin{pmatrix} ~~~83 & ~~~296 & -128 \\ ~~~296 & ~~~473 & -152 \\ -128 & -152 & ~~~335\end{pmatrix}
$$
Using any vector $\vec{v}_0$ in $\mathbb{R}^3$ as a starting guess, perform power iteration on **A** to find the doiminant eigenvector $\vec{v}$. Stop computing when  $||\vec{v}_{i} - \vec{v}_{i-1}||_2 < 0.5 \times 10^{-6}$. Then estimate the largest (in magnitude) eigenvalue $\lambda$ as follows. Take the definition of an eigenpair, left multiply each side by $\vec{v}^T$, and solve for $\lambda$, that is:

$$
\begin{eqnarray*}
\mat{A} \vec{v} & = & \lambda \vec{v}\\
\vec{v}^T \mat{A} \vec{v} & = & \lambda \vec{v}^T \vec{v} \\
\lambda & = & \frac{\vec{v}^T \mat{A} \vec{v}}{\vec{v}^T \vec{v}}.
\end{eqnarray*}
$$

### Problem 1 Solution

Your solution goes here.

### Problem 2

Every position in the United States House of Representatives is up for election every two years. A linear algebra student built the following (admittedly oversimplified, but still fun) model for predicting the political party of the winning candidate in a local congressional district:

- If our current representative is a Democrat, there is a 75% chance that a Democrat will win the next election, and a 25% chance that a Republican will win the next election.

- If our current representative is a Republican, there is a 50% chance that a Democrat will win the next election, and a 50% chance that a Republican will win the next election.

Note that this model assumes that the winner will always be a Democrat or Republican.

a. Let's model the political party of the current representative as a Markov chain with two states. Write down the transition matrix $\mat{P}$. Let the first state be Democrat.

b. We can represent the Markov chain as the dynamical system $\vec{x}_{k+1}=Ax_k$, where  
$x_k \in \mathbb{R}^2$ is a vector whose first component represents the probability that a Democrat wins the $k^{th}$ election, and the second component represents the probability that a Republican wins the $k^{th}$ election. What is the matrix $\mat{A}$ so that $x_{k+1}=\mat{A}x_k$? Hint: Double check your $\mat{A}$ on a simple example.

c. Let $k=0$ be the year 2020 election. A Democrat won the election last fall, so we'll take $\vec{x}_0=(1,0)^T$. According to the model, what is the probability that a Democrat will win the election in 2026? Hint: to raise a matrix $A$ to, for example, the power 2, make sure the `expm` package is installed, and type `A%^%2`. This function can't handle sparse matrices, however, so if $A$ were stored that way, you would need to write `as.matrix(A)%^%2`.

d. Let's introduce some helpful terminology. A matrix $\mat{M}$ is **primitive** if there exists a positive integer $k$ such that $\mat{M}^k$ has all positive entries. $\mat{M}$ is **stochastic** if its columns sum to one. There is a theorem called the **Perron-Frobenius Theorem** that says: if $\mat{M}$ is a nonnegative square primitive matrix, then there is a dominant eigenvalue $\lambda_1$ with eigenvector $\vec{v}_1$ that has all positive entries. If $\mat{M}$ is stochastic, then additionally, $\lambda_1 = 1$, so that $\mat{M}\vec{v}_1 = \vec{v}_1$. In the context of Markov chains, you can imagine $\vec{v}_1$ as being the (possibly scaled) stationary distribution of probabilities, that is, the probabilities of being in each state after many, many iterations of the Markov chain. Now apply this concept. In the limit of very long time, what is the probability that a Democrat will win the election? Would your answer be different if a Republican had won the last election?

### Problem 2 Solution

a. Your solution goes here.

b. Your solution goes here.

c. Your solution goes here.

d. Your solution goes here.

### Problem 3

a. Show that if $\mat{A}$ is an $n \times n$ invertible matrix and $\lambda$ is an eigenvalue of $\mat{A}$ with eigenvector $\vec{v}$, then $1/\lambda$ is an eigenvalue of $\mat{A}^{-1}$ with the same eigenvector $\vec{v}$. 

b. What happens if you apply the power iteration to $\mat{A}^{-1}$?

c. Let $\mat{A}$ be an $n \times n$ matrix, and let $\mat{C}=\mat{A}-s\mat{I}$, where $\mat{I}$ is the $n \times n$ identity matrix and $s$ is a scalar. Show that if $\lambda$ is an eigenvalue of $\mat{A}$ with eigenvector $\vec{v}$, then $\lambda-s$ is an eigenvalue of $\mat{C}$ with the same eigenvector $\vec{v}$. Note: $s$ is often called a **shift**.

d. Let's say you had a guess $\bar{\lambda}$ for an eigenvalue of $A$ and wanted to find the associated eigenvector. Use the previous two results to come up with a strategy.

### Problem 3 Solution

a. Your solution goes here.

b. Your solution goes here.

c. Your solution goes here.

d. Your solution goes here.

### Problem 4

Assume that $\mat{A}$ is a $5 \times 5$ matrix with eigenvalues $-5$, $-2$, $1/2$, $3/2$, $4$.

a. What eigenvalue is expected if you apply power iteration?

b. What eigenvalue is expected if you apply inverse power iteration?

c. What eigenvalue is expected if you apply inverse power iteration with shift $s=2$?

### Problem 4 Solution

a. Your solution goes here.

b. Your solution goes here.

c. Your solution goes here.