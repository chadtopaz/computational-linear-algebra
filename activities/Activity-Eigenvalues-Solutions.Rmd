---
title: "Activity - Eigenvalues"
author: "Solutions"
output: html_document
---

\renewcommand{\vec}[1]{\mathbf{#1}}
\renewcommand{\mat}[1]{\mathbf{#1}}

```{r message = FALSE, echo = FALSE}
library(pracma)
library(expm)
```

### Procedure

* Please collaborate on this activity with one person as scribe (typist). There should only be one laptop open. We're aiming for actual collaboration here, not two or three people working in parallel.

* It doesn't matter if you finish the whole activity during class.

* It **does** mater if you focus on helping each other learn. Helping each other learn is the point of this activity. Completing the problems is merely a mechanism to guide you towards learning.

* It **does** mater if everyone participates. The expectation is that every group member participates, where the definition of "participation" can include asking questions, answering questions, brainstorming, or any other form of meaningful engagement.

* In the last three minutes of class, before you leave, please knit your .Rmd file to html, print the html to .pdf, and have the scribe submit it as a group assignment through GLOW.

* Don't forget that the solutions are available on GLOW.

### Problem 0

Recalling that academic integrity policies apply even in an ungraded course, please disclose any irregular circumstances related to participation in your group (for instance, a group member was missing and did not contribute to this document).

### Problem 0 Solution

Your solution goes here.

### Problem 1

Let 
$$
\mat{A}=\frac{1}{9}\begin{pmatrix} ~~~83 & ~~~296 & -128 \\ ~~~296 & ~~~473 & -152 \\ -128 & -152 & ~~~335\end{pmatrix}
$$
Using any vector $\vec{v}_0$ in $\mathbb{R}^3$ as a starting guess, perform power iteration on **A** to find the doiminant eigenvector $\vec{v}$. Stop computing when  $||\vec{v}_{i} - \vec{v}_{i-1}||_2 < 0.5 \times 10^{-6}$. Then estimate the largest (in magnitude) eigenvalue $\lambda$ as follows. Take the definition of an eigenpair, left multiply each side by $\vec{v}^T$, and solve for $\lambda$, that is:

$$
\begin{eqnarray*}
\mat{A} \vec{v} & = & \lambda \vec{v}\\
\vec{v}^T \mat{A} \vec{v} & = & \lambda \vec{v}^T \vec{v} \\
\lambda & = & \frac{\vec{v}^T \mat{A} \vec{v}}{\vec{v}^T \vec{v}}.
\end{eqnarray*}
$$

### Problem 1 Solution

```{r}
A <- 1/9*cbind(c(83,296,-128),c(296,473,-152),c(-128,-152,335))
PI <- function(A,tol=.000005) {
  err <- Inf
  v <- runif(ncol(A))
  v <- v/Norm(v)
  while (err > tol) {
    vold <- v
    v <- A %*% v
    v <- v/Norm(v)
    err <- Norm(v-vold)
  }
  return(v)
}
print(v <- PI(A))
print(lambda <- (t(v)%*%A%*%v)/(t(v)%*%v))
```

### Problem 2

Every position in the United States House of Representatives is up for election every two years. A linear algebra student built the following (admittedly oversimplified, but still fun) model for predicting the political party of the winning candidate in a local congressional district:

- If our current representative is a Democrat, there is a 75% chance that a Democrat will win the next election, and a 25% chance that a Republican will win the next election.

- If our current representative is a Republican, there is a 50% chance that a Democrat will win the next election, and a 50% chance that a Republican will win the next election.

Note that this model assumes that the winner will always be a Democrat or Republican.

a. Let's model the political party of the current representative as a Markov chain with two states. Write down the transition matrix $\mat{P}$. Let the first state be Democrat.

b. We can represent the Markov chain as the dynamical system $\vec{x}_{k+1}=Ax_k$, where  
$x_k \in \mathbb{R}^2$ is a vector whose first component represents the probability that a Democrat wins the $k^{th}$ election, and the second component represents the probability that a Republican wins the $k^{th}$ election. What is the matrix $\mat{A}$ so that $x_{k+1}=\mat{A}x_k$? Hint: Double check your $\mat{A}$ on a simple example.

c. Let $k=0$ be the year 2020 election. A Democrat won the election last fall, so we'll take $\vec{x}_0=(1,0)^T$. According to the model, what is the probability that a Democrat will win the election in 2026? Hint: to raise a matrix $A$ to, for example, the power 2, make sure the `expm` package is installed, and type `A%^%2`. This function can't handle sparse matrices, however, so if $A$ were stored that way, you would need to write `as.matrix(A)%^%2`.

d. Let's introduce some helpful terminology. A matrix $\mat{M}$ is **primitive** if there exists a positive integer $k$ such that $\mat{M}^k$ has all positive entries. $\mat{M}$ is **stochastic** if its columns sum to one. There is a theorem called the **Perron-Frobenius Theorem** that says: if $\mat{M}$ is a nonnegative square primitive matrix, then there is a dominant eigenvalue $\lambda_1$ with eigenvector $\vec{v}_1$ that has all positive entries. If $\mat{M}$ is stochastic, then additionally, $\lambda_1 = 1$, so that $\mat{M}\vec{v}_1 = \vec{v}_1$. In the context of Markov chains, you can imagine $\vec{v}_1$ as being the (possibly scaled) stationary distribution of probabilities, that is, the probabilities of being in each state after many, many iterations of the Markov chain. Now apply this concept. In the limit of very long time, what is the probability that a Democrat will win the election? Would your answer be different if a Republican had won the last election?

### Problem 2 Solution

a. 
```{r}
P <- rbind(c(.75,.25),c(.5,.5))
```

b.
```{r}
A <- t(P)
```

After the current election, there is probability 1 that a democrat is in office, so we can check the matrix $\mat{A}$ by letting $\vec{x}_0 = (1,0)^T$ and computing $\mat{A}\vec{x}_0$:

```{r}
x0 <- c(1,0)
A%*%x0
```

Indeed it shows there is a 75% chance of a democrat winning the next election.

c. The election in 2026 is the third election, so we want to examine $\mat{A}^3 \vec{x}_0$:
```{r}
A%^%3%*%x0
```

So there is a 67.2% chance that a democrat wins the 2026 election according to this model.

d. Note that $\mat{A}$ is nonnegative. It is primitive because $\mat{A}^1$ is a positive matrix. It is stochastic because its columns sum to 1:
```{r}
colSums(A)
```
To find the long-term probabilities, we can look at the dominant eigenvector of $\mat{A}$. Note that `R` normalizes this vector to be length 1, but we want to normalize it to sum to one to get a probability distribution. Since our matrix is small, feel free to use R's built-in `eigen` command to find the eigenvectors.
```{r}
out <- eigen(A)
long.term.prob <- out$vectors[,1]/sum(out$vectors[,1])
print(long.term.prob)
```
So there is an approximately 2/3 chance a democrat wins the election far in the future. By the Perron-Frobenius theorem, our answer wouldn't depend on the initial state of the Markov chain.

### Problem 3

a. Show that if $\mat{A}$ is an $n \times n$ invertible matrix and $\lambda$ is an eigenvalue of $\mat{A}$ with eigenvector $\vec{v}$, then $1/\lambda$ is an eigenvalue of $\mat{A}^{-1}$ with the same eigenvector $\vec{v}$. 

b. What happens if you apply the power iteration to $\mat{A}^{-1}$?

c. Let $\mat{A}$ be an $n \times n$ matrix, and let $\mat{C}=\mat{A}-s\mat{I}$, where $\mat{I}$ is the $n \times n$ identity matrix and $s$ is a scalar. Show that if $\lambda$ is an eigenvalue of $\mat{A}$ with eigenvector $\vec{v}$, then $\lambda-s$ is an eigenvalue of $\mat{C}$ with the same eigenvector $\vec{v}$. Note: $s$ is often called a **shift**.

d. Let's say you had a guess $\bar{\lambda}$ for an eigenvalue of $A$ and wanted to find the associated eigenvector. Use the previous two results to come up with a strategy.

### Problem 3 Solution

a.

$$ \mat{A}\vec{v}=\lambda \vec{v} \Rightarrow \mat{A}^{-1}\mat{A}\vec{v} = \lambda \mat{A}^{-1}\vec{v} \Rightarrow \frac{1}{\lambda}\vec{v}=\mat{A}^{-1}\vec{v}.$$

b. You'll converge to the eigenvector associated with the eigenvalue of $\mat{A}$ that is smallest in magnitude.

c.

$$\mat{C}\vec{v}=(A-s\mat{I})\vec{v}=\mat{A}\vec{v}-s\vec{v}=\lambda \vec{v} - s\vec{v} = (\lambda-s)\vec{v}.$$

d. Let $\bar{\lambda}$ be your shift $s$ from part (c) above, run the inverse power iteration on $\mat{C}=\mat{A}-\bar{\lambda}\mat{I}$, and you'll find the associated eigenvector. To find the actual eigenvalue, you'll have to add $\bar{\lambda}$ back to whatever eigenvalue you found in the inverse power iteration.

### Problem 4

Assume that $\mat{A}$ is a $5 \times 5$ matrix with eigenvalues $-5$, $-2$, $1/2$, $3/2$, $4$.

a. What eigenvalue is expected if you apply power iteration?

b. What eigenvalue is expected if you apply inverse power iteration?

c. What eigenvalue is expected if you apply inverse power iteration with shift $s=2$?

### Problem 4 Solution

a. Power iternation will converge to $-5$ since it is the eigenvalue largest in magntidue.

b. We consider the reciprocals of the eigenvalues, namely $-1/5$, $-1/2$, $2$, $2/3$, $1/2$. The largest is $2$. Therefore, power iteration on $\mat{A}^{-1}$ will converge to $2$, which corresponds to an eigenvalue of $\mat{A}$ equal to $1/2$.

c. First, the shift of $2$ shifts the eigenvalues of $\mat{A}$ to $-7$, $-4$, $-3/2$, $-1/2$, $2$. The largest of the reciprocals of these is $-2$, so inverse power iteration on the shifted matrix will converge to $-1/2$. After shifting back by 2, we find an eigenvalue of $3/2$.